---
title: OpenAI
description: Integration with OpenAI's API services, supporting both completion and embedding models.
---

import { Cards } from 'nextra/components'

# OpenAI Integration

The OpenAI provider in Rig offers integration with OpenAI's API services, supporting both completion and embedding models. It provides a client-based architecture for interacting with OpenAI's models.

To use the OpenAI module, you need an OpenAI key. You can also alternatively use a custom self-hosted instance that implements the Chat Completions API to mimic OpenAI, should you want to use a service that is compatible with the OpenAI public APIs.

## Basic Usage
To use the OpenAI module, make sure `rig-core` is added as a dependency to your Rust project (alongside `tokio` with the `macros` and `rt-multi-thread` for the purposes of the example if you want to run the code below). Then you can simply import the OpenAI provider and use it as below.

> **Note**: Make sure your environment variable `OPENAI_API_KEY` is set before running the example code.

```rust
use rig::providers::openai;

// Create client from environment variable
let client = openai::Client::from_env();

// Or explicitly with API key
let client = openai::Client::new("your-api-key");

// Create a completion model
let gpt4 = client.completion_model(openai::GPT_4);

// Create an embedding model
let embedder = client.embedding_model(openai::TEXT_EMBEDDING_3_LARGE);
```

## Available Models
Below is a list of available models that have been added as const variables to the provider module. If there is a supported model you would like to use but is not provided below, you can simply use a string literal instead.

### Completion Models
- `GPT_4` / `GPT_4O`: GPT-4 base and optimized versions
- `GPT_35_TURBO`: GPT-3.5 Turbo and its variants
- `GPT_35_TURBO_INSTRUCT`: Instruction-tuned GPT-3.5

### Embedding Models
- `TEXT_EMBEDDING_3_LARGE`: 3072 dimensions
- `TEXT_EMBEDDING_3_SMALL`: 1536 dimensions
- `TEXT_EMBEDDING_ADA_002`: 1536 dimensions (legacy)

## Supported Features

### Completions
Find out more about the OpenAI chat completions API [here.](https://platform.openai.com/docs/api-reference/chat)

### Embeddings
Find out more about the embeddings API [here.](https://platform.openai.com/docs/api-reference/embeddings)

### Tool usage
OpenAI also supports tool usage. Rig exposes a public unified API for tool creation and usage that you can find more about here. TODO: Add link

## Special Considerations

### Using a custom self-hosted instance
The OpenAI Chat Completions API is extremely popular and has become a gold standard across many, many API services created by other companies that offer LLM services. Below is an example of how you can use Ollama with the OpenAI provider:

```rust
/// This example requires that you have the [`ollama`](https://ollama.com) server running locally.
use rig::{completion::Prompt, providers};

#[tokio::main]
async fn main() -> Result<(), anyhow::Error> {
    // Create an OpenAI client with a custom base url, a local ollama endpoint
    // The API Key is unnecessary for most local endpoints
    let client = providers::openai::Client::from_url("ollama", "http://localhost:11434/v1");

    // Create agent with a single context prompt
    let comedian_agent = client
        .agent("llama3.2:latest")
        .preamble("You are a comedian here to entertain the user using humour and jokes.")
        .build();

    // Prompt the agent and print the response
    let response = comedian_agent.prompt("Entertain me!").await?;
    println!("{}", response);

    Ok(())
}
```


<br />

<Cards>
<Cards.Card title="OpenAIAPI Reference" href="https://platform.openai.com/docs/api-reference/introduction"/>
<Cards.Card title="API Reference (docs.rs)" href="https://docs.rs/rig-core/latest/rig/providers/openai/index.html"/>
</Cards>
